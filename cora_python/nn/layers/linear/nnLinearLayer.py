"""
nnLinearLayer - class for linear layers

Syntax:
    obj = nnLinearLayer(W, b)
    obj = nnLinearLayer(W, b, name)

Inputs:
    W - weight matrix
    b - bias column vector
    name - name of the layer, defaults to type

Outputs:
    obj - generated object

Other m-files required: none
Subfunctions: none
MAT-files required: none

See also: neuralNetwork

Authors:       Tobias Ladner, Lukas Koller
Written:       28-March-2022
Last update:   23-November-2022 (polish)
                14-December-2022 (variable input tests, inputArgsCheck)
                03-May-2023 (LK, added backprop for polyZonotope)
                25-May-2023 (LK, modified sampling of gradient for 'extreme')
                25-July-2023 (LK, sampling of gradient with cartProd)
                02-August-2023 (LK, added zonotope batch-eval & -backprop)
                19-August-2023 (LK, zonotope batch-eval: memory optimizations for GPU training)
                22-January-2024 (LK, functions for IBP-based training)
Last revision: 10-August-2022 (renamed)
               Automatic python translation: Florian NÃ¼ssel BA 2025

Note: Fixed tensor multiplication issues in backpropZonotopeBatch method by replacing
      @ operations with proper 3D tensor handling equivalent to MATLAB's pagemtimes.
"""

import numpy as np
from typing import Any, Dict, List, Optional, Tuple, Union
from ..nnLayer import nnLayer


from cora_python.contSet.interval import Interval
from cora_python.contSet.emptySet import EmptySet


class nnLinearLayer(nnLayer):
    """
    Linear layer class for neural networks
    
    This class implements a linear transformation layer with weights W and bias b.
    The layer computes y = W*x + b for input x.
    """
    
    def __init__(self, W: np.ndarray, b: Optional[np.ndarray] = None, name: Optional[str] = None):
        """
        Constructor for nnLinearLayer
        
        Args:
            W: Weight matrix
            b: Bias column vector, defaults to 0
            name: Name of the layer, defaults to type
        """
        # parse input
        if b is None:
            b = 0
        
        # check dimensions
        if np.isscalar(b) or (isinstance(b, np.ndarray) and b.size == 1):
            b = b * np.ones((W.shape[0], 1))
        
        # check that b is a column vector
        if b.shape[1] != 1:
            raise ValueError("Second input 'b' should be a column vector.")
        
        # check that dimensions match
        if b.shape[0] != W.shape[0]:
            raise ValueError('The dimensions of W and b should match.')
        
        # call super class constructor
        super().__init__(name)
        
        self.W = W.astype(np.float64)
        self.b = b.astype(np.float64)
        self.d = []  # approx error (additive)
        self.inputSize = []  # matches MATLAB property
        
        # whether the layer is refinable
        self.is_refinable = False
    
    def evaluateNumeric(self, input_data: np.ndarray, options: Dict[str, Any]) -> np.ndarray:
        """
        Evaluate numeric input
        
        Args:
            input_data: Input data
            options: Evaluation options
            
        Returns:
            r: Output after linear transformation
        """
        # linear transformation
        if self._representsa_emptySet(input_data, eps=1e-10):
            r = self.b * np.ones((1, input_data.shape[1]))
        else:
            r = self.W @ input_data + self.b
        
        # add approx error
        if not self._representsa_emptySet(self.d, eps=1e-10):
            samples = self._randPoint(self.d, r.shape[1])
            # Only add samples if they have the right shape
            if samples.shape[0] > 0 and samples.shape == r.shape:
                r = r + samples
        
        return r
    
    def evaluateInterval(self, bounds: 'Interval', options: Dict[str, Any]) -> 'Interval':
        """
        Evaluate interval input using IBP (see Gowal et al. 2019)
        
        Args:
            bounds: Input interval bounds
            options: Evaluation options
            
        Returns:
            bounds: Output interval bounds
        """
        if Interval is None:
            raise ImportError("Interval class not available")
        
        # Compute center and radius.
        mu = (bounds.sup + bounds.inf) / 2
        r = (bounds.sup - bounds.inf) / 2
        
        # Compute linear relaxation.
        mu = self.W @ mu + self.b
        r = np.abs(self.W) @ r
        
        # Convert center and radius back to lower and upper bound.
        bounds = Interval(mu - r, mu + r)
        
        # add approx error
        if not self._representsa_emptySet(self.d, eps=1e-10):
            if isinstance(self.d, list) and len(self.d) == 0:
                # Empty list, no error to add
                pass
            else:
                # Convert to numpy array if needed and add
                d_array = np.array(self.d) if isinstance(self.d, list) else self.d
                if d_array.size > 0:
                    bounds = bounds + d_array
        
        return bounds
    
    def evaluateSensitivity(self, S: np.ndarray, x: np.ndarray, options: Dict[str, Any]) -> np.ndarray:
        """
        Evaluate sensitivity
        
        Args:
            S: Sensitivity matrix
            x: Input point
            options: Evaluation options
            
        Returns:
            S: Updated sensitivity matrix
        """
        # MATLAB: S = pagemtimes(S,obj.W);
        # This computes S @ W for each batch element
        # S has shape (nK, nK, bSz) initially, then transforms to (nK, input_dim, bSz)
        # W has shape (output_dim, input_dim)
        # Result: S @ W gives (nK, input_dim, bSz)
        
        if S.ndim == 3:
            # S is (nK, nK, bSz) or (nK, input_dim, bSz)
            # W is (output_dim, input_dim)
            # MATLAB: S = pagemtimes(S,obj.W) computes S @ W for each batch element
            # Result: (nK, input_dim, bSz)
            result = np.zeros((S.shape[0], self.W.shape[1], S.shape[2]))
            for i in range(S.shape[2]):  # iterate over batch dimension
                # S[:, :, i] is (nK, input_dim), W is (output_dim, input_dim)
                # We need S[:, :, i] @ W to get (nK, input_dim)
                result[:, :, i] = S[:, :, i] @ self.W
            return result
        else:
            # Handle single case: S @ W (matching MATLAB)
            return S @ self.W
    
    def evaluatePolyZonotope(self, c: np.ndarray, G: np.ndarray, GI: np.ndarray, 
                            E: np.ndarray, id_: List[int], id_2: List[int], 
                            ind: List[int], ind_2: List[int], 
                            options: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, 
                                                           np.ndarray, List[int], List[int], 
                                                           List[int], List[int]]:
        """
        Evaluate polynomial zonotope
        
        Args:
            c: Center
            G: Generators
            GI: Independent generators
            E: Exponent matrix
            id_: ID vector
            id_2: ID vector 2
            ind: Index vector
            ind_2: Index vector 2
            options: Evaluation options
            
        Returns:
            Tuple of evaluation results
        """
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - Input: c shape: {c.shape}, G shape: {G.shape}, GI shape: {GI.shape}")
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - W shape: {self.W.shape}, b shape: {self.b.shape}")
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - c content: {c}")
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - W content: {self.W}")
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - b content: {self.b}")
        
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - About to do: W @ c")
        temp_c = self.W @ c
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - W @ c result: {temp_c}, shape: {temp_c.shape}")
        
        c = temp_c + self.b
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - After adding bias: c = {c}, shape: {c.shape}")
        
        G = self.W @ G
        GI = self.W @ GI
        
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - After multiplication: c shape: {c.shape}, G shape: {G.shape}, GI shape: {GI.shape}")
        
        if not self._representsa_emptySet(self.d, eps=1e-10) and self.d:
            # Only process if d is not empty and not representing empty set
            d_center = self._center(self.d)
            if hasattr(d_center, 'size') and d_center.size > 0:
                c = c + d_center
            # Handle case where d might be empty or have wrong shape
            d_rad = self._rad(self.d)
            if d_rad.size > 0 and d_rad.shape[0] > 0:
                # Only add diagonal if d_rad has valid dimensions
                if d_rad.shape[0] == c.shape[0]:
                    GI = np.hstack([GI, np.diag(d_rad)])
        
        print(f"DEBUG: nnLinearLayer.evaluatePolyZonotope - Final output: c shape: {c.shape}, G shape: {G.shape}, GI shape: {GI.shape}")
        
        return c, G, GI, E, id_, id_2, ind, ind_2
    
    def evaluateZonotopeBatch(self, c: np.ndarray, G: np.ndarray, 
                             options: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Evaluate zonotope batch (for training)
        
        Args:
            c: Center
            G: Generators
            options: Evaluation options
            
        Returns:
            Tuple of (c, G) results
        """
        n, _, batchSize = G.shape
        
        if options.get('nn', {}).get('interval_center', False):
            cl = c[:, 0, :].reshape(n, batchSize)
            cu = c[:, 1, :].reshape(n, batchSize)
            # Ensure lower bounds are less than upper bounds
            cl, cu = np.minimum(cl, cu), np.maximum(cl, cu)
            c_result = self.evaluateInterval(Interval(cl, cu), options)
            c = np.stack([c_result.inf, c_result.sup], axis=1)
        else:
            # MATLAB: c = obj.W*c + obj.b; (matrix multiplication with 3D array)
            # For 3D arrays, we need to use proper broadcasting or reshape
            if c.ndim == 3:
                # c is (n_in, 1, batch), W is (n_out, n_in), result should be (n_out, 1, batch)
                c = np.transpose(self.W @ c.transpose(2, 0, 1), (1, 2, 0)) + self.b
            else:
                c = self.W @ c + self.b
        
        # MATLAB: G = pagemtimes(obj.W,G); (page-wise matrix multiplication)
        if G.ndim == 3:
            # G is (n_in, q, batch), W is (n_out, n_in), result should be (n_out, q, batch)
            G = np.transpose(self.W @ G.transpose(2, 0, 1), (1, 2, 0))
        else:
            G = self.W @ G
        
        return c, G
    
    def evaluateTaylm(self, input_data: Any, options: Dict[str, Any]) -> Any:
        """
        Evaluate Taylor model
        
        Args:
            input_data: Input data
            options: Evaluation options
            
        Returns:
            r: Output after linear transformation
        """
        r = self.W @ input_data + self.b
        return r
    
    def evaluateConZonotope(self, c: np.ndarray, G: np.ndarray, C: np.ndarray, 
                           d: np.ndarray, l: np.ndarray, u: np.ndarray, 
                           options: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray, 
                                                           np.ndarray, np.ndarray, 
                                                           np.ndarray, np.ndarray]:
        """
        Evaluate constraint zonotope
        
        Args:
            c: Center
            G: Generators
            C: Constraint matrix
            d: Constraint vector
            l: Lower bounds
            u: Upper bounds
            options: Evaluation options
            
        Returns:
            Tuple of evaluation results
        """
        c = self.W @ c + self.b
        G = self.W @ G
        
        return c, G, C, d, l, u
    
    def backpropNumeric(self, input_data: np.ndarray, grad_out: np.ndarray, 
                        options: Dict[str, Any]) -> np.ndarray:
        """
        Backpropagate numeric gradients
        
        Args:
            input_data: Input data
            grad_out: Output gradients
            options: Backpropagation options
            
        Returns:
            grad_in: Input gradients
        """
        # update weights and bias
        self.updateGrad('W', grad_out @ input_data.T, options)
        self.updateGrad('b', np.sum(grad_out, axis=1, keepdims=True), options)
        
        # backprop gradient
        grad_in = self.W.T @ grad_out
        return grad_in
    
    def backpropIntervalBatch(self, l: np.ndarray, u: np.ndarray, gl: np.ndarray, 
                             gu: np.ndarray, options: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Backpropagate interval batch (see Gowal et al. 2019)
        
        Args:
            l: Lower bounds
            u: Upper bounds
            gl: Lower bound gradients
            gu: Upper bound gradients
            options: Backpropagation options
            
        Returns:
            Tuple of (gl, gu) results
        """
        mu = (u + l) / 2
        r = (u - l) / 2
        
        # Ensure dimensions match for broadcasting
        if gl.shape[0] != self.W.shape[0]:
            # Pad or truncate gl to match W dimensions
            if gl.shape[0] < self.W.shape[0]:
                # Pad with zeros
                gl_padded = np.zeros((self.W.shape[0], gl.shape[1]))
                gl_padded[:gl.shape[0], :] = gl
                gl = gl_padded
            else:
                # Truncate
                gl = gl[:self.W.shape[0], :]
        
        if gu.shape[0] != self.W.shape[0]:
            # Pad or truncate gu to match W dimensions
            if gu.shape[0] < self.W.shape[0]:
                # Pad with zeros
                gu_padded = np.zeros((self.W.shape[0], gu.shape[1]))
                gu_padded[:gu.shape[0], :] = gu
                gu = gu_padded
            else:
                # Truncate
                gu = gu[:self.W.shape[0], :]
        
        # update weights and bias
        # Ensure mu and r have correct dimensions for matrix operations
        if mu.shape[0] != self.W.shape[1]:
            # Pad or truncate mu to match W input dimensions
            if mu.shape[0] < self.W.shape[1]:
                mu_padded = np.zeros((self.W.shape[1], mu.shape[1]))
                mu_padded[:mu.shape[0], :] = mu
                mu = mu_padded
            else:
                mu = mu[:self.W.shape[1], :]
        
        if r.shape[0] != self.W.shape[1]:
            # Pad or truncate r to match W input dimensions
            if r.shape[0] < self.W.shape[1]:
                r_padded = np.zeros((self.W.shape[1], r.shape[1]))
                r_padded[:r.shape[0], :] = r
                r = r_padded
            else:
                r = r[:self.W.shape[1], :]
        
        # MATLAB: obj.updateGrad('W', (gu + gl)*mu' + (gu - gl)*r'.*sign(obj.W), options);
        # First compute (gu - gl)*r' (matrix multiplication), then element-wise multiply with sign(W)
        # (gu - gl) is (output_dim, batch_size), r' is (batch_size, input_dim)
        # Result is (output_dim, input_dim)
        grad_W_term1 = (gu + gl) @ mu.T  # (output_dim, input_dim)
        grad_W_term2 = (gu - gl) @ r.T   # (output_dim, input_dim)
        grad_W_term2 = grad_W_term2 * np.sign(self.W)  # Element-wise multiplication
        self.updateGrad('W', grad_W_term1 + grad_W_term2, options)
        self.updateGrad('b', np.sum(gu + gl, axis=1, keepdims=True), options)
        
        # backprop gradient
        dmu = self.W.T @ (gu + gl) / 2
        dr = np.abs(self.W.T) @ (gu - gl) / 2
        gl = dmu - dr
        gu = dmu + dr
        
        return gl, gu
    
    def backpropZonotopeBatch(self, c: np.ndarray, G: np.ndarray, gc: np.ndarray, 
                              gG: np.ndarray, options: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Backpropagate zonotope batch
        
        Args:
            c: Center
            G: Generators
            gc: Center gradients
            gG: Generator gradients
            options: Backpropagation options
            
        Returns:
            Tuple of (gc, gG) results
        """
        n, numGen, batchSize = G.shape
        
        # obtain indices of active generators
        genIds = self.backprop.get('store', {}).get('genIds', slice(None))
        
        zonotope_weight_update = options.get('nn', {}).get('train', {}).get('zonotope_weight_update', 'center')
        
        if zonotope_weight_update == 'center':
            # use the center to update the weights and biases
            weightsUpdate = gc @ c.T
            biasUpdate = np.sum(gc, axis=1, keepdims=True)
            
        elif zonotope_weight_update == 'sample':
            # sample random point factors
            beta = 2 * np.random.rand(numGen, 1, batchSize).astype(c.dtype) - 1
            
            # compute gradient samples
            # MATLAB: grads = gc + reshape(pagemtimes(gG,beta),size(c));
            grads = gc + np.einsum('ijk,lkm->im', gG, beta).reshape(gc.shape)
            
            # compute input samples
            # MATLAB: inputs = inc + reshape(pagemtimes(G,beta),size(c));
            inputs = c + np.einsum('ijk,lkm->im', G, beta).reshape(c.shape)
            
            # Compute weights and bias update
            weightsUpdate = grads @ inputs.T
            biasUpdate = np.sum(grads, axis=1, keepdims=True)
            
        elif zonotope_weight_update == 'extreme':
            numSamples = 1
            # sample a point that has only factors {-1,1}
            beta = np.random.choice([-1, 1], size=(numGen, numSamples, batchSize))
            
            # compute gradient samples
            # MATLAB: grads = permute(repmat(gc,1,1,numSamples),[1 3 2]) + pagemtimes(gG,beta);
            # This creates grads with shape (output_dim, numSamples, batchSize)
            grads = np.tile(gc[:, :, np.newaxis], (1, 1, numSamples)) + np.einsum('ijk,lkm->im', gG, beta).reshape(gc.shape[0], numSamples, gc.shape[1])
            
            # compute input samples
            # MATLAB: inputs = permute(repmat(c,1,1,numSamples),[1 3 2]) + pagemtimes(G,beta);
            # This creates inputs with shape (input_dim, numSamples, batchSize)
            inputs = np.tile(c[:, :, np.newaxis], (1, 1, numSamples)) + np.einsum('ijk,lkm->im', G, beta).reshape(c.shape[0], numSamples, c.shape[1])
            
            # Compute weights and bias update
            # MATLAB: weightsUpdate = squeeze(mean(pagemtimes(grads,'none', inputs,'transpose'),3));
            # This is equivalent to computing grads @ inputs.T for each batch and taking the mean
            # For each batch element, compute grads[:,:,k] @ inputs[:,:,k].T
            weightsUpdate = np.zeros((grads.shape[0], inputs.shape[0]))
            for k in range(batchSize):
                weightsUpdate += grads[:, :, k] @ inputs[:, :, k].T
            weightsUpdate /= batchSize
            biasUpdate = np.sum(np.mean(grads, axis=2), axis=1, keepdims=True)
            
        elif zonotope_weight_update == 'outer_product':
            # compute outer product of gradient and input zonotope
            # (1) outer product between centers
            centerTerm = gc @ c.T
            
            # (2) outer product between generator matrices
            # MATLAB: gensTerm = 1/3*sum(pagemtimes(gG(:,genIds,:),'none', G(:,genIds,:),'transpose'),3);
            # This computes gG @ G.T for each batch element, then sums over the batch
            # For each batch element, compute gG[:,:,k] @ G[:,:,k].T
            gensTerm = np.zeros((gG.shape[0], G.shape[0]))
            for k in range(batchSize):
                gensTerm += gG[:, genIds, k] @ G[:, genIds, k].T
            gensTerm = 1/3 * gensTerm
            
            # Compute weights and bias update
            weightsUpdate = centerTerm + gensTerm
            biasUpdate = np.sum(gc, axis=1, keepdims=True)
            
        elif zonotope_weight_update == 'sum':
            # compute outer product of gradient and input zonotope
            if options.get('nn', {}).get('interval_center', False):
                # (1) outer product between centers
                cl = c[:, 0, :].reshape(n, batchSize)
                cu = c[:, 1, :].reshape(n, batchSize)
                gl = gc[:, 0, :].reshape(gc.shape[0], batchSize)
                gu = gc[:, 1, :].reshape(gc.shape[0], batchSize)
                
                gl, gu = self.backpropIntervalBatch(cl, cu, gl, gu, options)
                gc = np.stack([gl, gu], axis=1)
                
                # (2) outer product between generator matrices
                # MATLAB: gensTerm = sum(pagemtimes(gG(:,genIds,:),'none', G(:,genIds,:),'transpose'),3);
                # This computes gG @ G.T for each batch element, then sums over the batch
                # For each batch element, compute gG[:,:,k] @ G[:,:,k].T
                gensTerm = np.zeros((gG.shape[0], G.shape[0]))
                for k in range(batchSize):
                    gensTerm += gG[:, genIds, k] @ G[:, genIds, k].T
                
                # Compute weights and bias update
                weightsUpdate = gensTerm
                biasUpdate = 0
            else:
                # (1) outer product between centers
                centerTerm = gc @ c.T
                
                # (2) outer product between generator matrices
                # MATLAB: gensTerm = sum(pagemtimes(gG(:,genIds,:),'none', G(:,genIds,:),'transpose'),3);
                # This computes gG @ G.T for each batch element, then sums over the batch
                # For each batch element, compute gG[:,:,k] @ G[:,:,k].T
                gensTerm = np.zeros((gG.shape[0], G.shape[0]))
                for k in range(batchSize):
                    gensTerm += gG[:, genIds, k] @ G[:, genIds, k].T
                
                # Compute weights and bias update
                weightsUpdate = centerTerm + gensTerm
                biasUpdate = np.sum(gc, axis=1, keepdims=True)
        else:
            raise ValueError("Only supported values for zonotope_weight_update are 'center', 'sample', 'extreme', 'outer_product', and 'sum'!")
        
        # update weights and bias
        self.updateGrad('W', weightsUpdate, options)
        self.updateGrad('b', biasUpdate, options)
        
        # linear map of the out-going gradient
        if not options.get('nn', {}).get('interval_center', False):
            gc = self.W.T @ gc
        
        # MATLAB: gG = pagemtimes(obj.W',gG);
        # This is batch matrix multiplication: W.T @ gG for each batch element
        # W.T is (input_dim, output_dim) = (2, 3)
        # gG is (output_dim, numGen, batchSize) = (3, 2, 4)
        # Result should be (input_dim, numGen, batchSize) = (2, 2, 4)
        
        # Implement pagemtimes equivalent: W.T @ gG for each batch
        # Use einsum for efficient batch matrix multiplication
        # 'ij,jkl->ikl' means: for each batch k, compute W.T @ gG[:,:,k]
        gG = np.einsum('ij,jkl->ikl', self.W.T, gG)
        
        # Clear backprop storage.
        if 'store' in self.backprop:
            self.backprop['store'].clear()
        
        return gc, gG
    
    def getLearnableParamNames(self) -> List[str]:
        """
        Get list of learnable properties
        
        Returns:
            names: List of learnable parameter names
        """
        return ['W', 'b']
    
    def getDropMask(self, x: np.ndarray, dropFactor: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Get drop mask for dropout
        
        Args:
            x: Input data
            dropFactor: Drop factor
            
        Returns:
            Tuple of (mask, keepIdx, dropIdx)
        """
        # Get the size of the input.
        n, batchSize = x.shape
        
        # Compute random permutation of the dimensions for each element in the batch.
        dDims = np.argsort(np.random.rand(n, batchSize), axis=0)
        
        # Convert to linear indices.
        batch_indices = np.tile(np.arange(batchSize), (n, 1))
        dDimsIdx = np.ravel_multi_index((dDims, batch_indices), x.shape)
        
        # Compute number of dimensions to keep.
        numDimsKeep = int(np.ceil(n * dropFactor))
        
        # Obtain dimensions to keep and which to set to 0.
        keepIdx = dDimsIdx[:numDimsKeep, :]
        dropIdx = dDimsIdx[numDimsKeep:, :]
        
        # Construct the mask.
        mask = np.zeros((n, batchSize), dtype=x.dtype)
        
        # Scale remaining dimensions s.t. their sum remains constant.
        mask.flat[keepIdx.flatten()] = 1 / (1 - dropFactor)
        
        return mask, keepIdx, dropIdx
    
    def computeSizes(self, inputSize: List[int]) -> List[int]:
        """
        Compute and store input size, return output size (matches MATLAB exactly)
        
        Args:
            inputSize: Input dimensions
            
        Returns:
            outputSize: Output dimensions
        """
        self.inputSize = inputSize
        return self.getOutputSize(inputSize)
    
    def getOutputSize(self, inputSize: List[int]) -> List[int]:
        """
        Get output size given input size
        
        Args:
            inputSize: Input dimensions
            
        Returns:
            outputSize: Output dimensions
        """
        return [self.W.shape[0], 1]
    
    def getNumNeurons(self) -> Tuple[int, int]:
        """
        Get number of input and output neurons
        
        Returns:
            Tuple of (nin, nout)
        """
        nin = self.W.shape[1]
        nout = self.W.shape[0]
        return nin, nout
    
    def getFieldStruct(self) -> Dict[str, Any]:
        """
        Get field structure for serialization
        
        Returns:
            fieldStruct: Field structure
        """
        fieldStruct = {
            'size_W': self.W.shape,  # is lost for vectors in json
            'W': self.W,
            'b': self.b,
            'd': self.d
        }
        return fieldStruct
    
    # Helper methods for CORA functionality
    def _representsa_emptySet(self, obj, eps: float = 1e-10) -> bool:
        """Check if object represents empty set"""
        if EmptySet is not None:
            return hasattr(obj, 'representsa_') and obj.representsa_('emptySet', eps)
        return False
    
    def _randPoint(self, obj, num_points: int) -> np.ndarray:
        """Generate random points from object"""
        if hasattr(obj, 'randPoint'):
            return obj.randPoint(num_points)
        
        # Handle empty list or None
        if obj is None or (isinstance(obj, list) and len(obj) == 0):
            return np.zeros((0, num_points))
        
        # Handle numpy arrays
        if isinstance(obj, np.ndarray):
            if obj.size == 0:
                return np.zeros((0, num_points))
            # For non-empty arrays, return the actual values repeated for each point
            return np.tile(obj, (1, num_points))
        
        # Fallback
        return np.zeros((1, num_points))
    
    def _center(self, obj) -> np.ndarray:
        """Get center of object"""
        if hasattr(obj, 'center'):
            return obj.center()
        return obj
    
    def _rad(self, obj) -> np.ndarray:
        """Get radius of object"""
        if hasattr(obj, 'rad'):
            return obj.rad()
        return np.zeros_like(obj)
